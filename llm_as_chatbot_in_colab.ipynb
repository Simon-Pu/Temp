{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOk1wzSJwjWeFF1CKecHtZN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simon-Pu/Temp/blob/master/llm_as_chatbot_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM as a Chatbot Service\n",
        "https://github.com/deep-diver/LLM-As-Chatbot"
      ],
      "metadata": {
        "id": "B-wAtzLpd2G8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check GPU's Memory Capacity\n",
        "By running nvidia-smi command, you can find out the GPU's memory capacity on the current system.\n",
        "\n",
        "With the standard GPU instance(___T4___) which is free, you can run 7B and 13B models. With the premium GPU instance(___A100 40GB___) which is paid with the compute unit that you own, you can even run 30B model! Choose the instance at the menu Runtime -> Change runtime type -> Hardware accelerator (GPU) -> GPU class (Standard or Premium)"
      ],
      "metadata": {
        "id": "LSWZAWDAdD-1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_ugQ6pLUgwN"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone the repository"
      ],
      "metadata": {
        "id": "JcaB5EJcdNOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deep-diver/LLM-As-Chatbot.git"
      ],
      "metadata": {
        "id": "x_n17oWwUybQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Move into the directory of the cloned repository"
      ],
      "metadata": {
        "id": "No879xo7dTjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLM-As-Chatbot"
      ],
      "metadata": {
        "id": "PRwpWNQSU3j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "etJcclRQdZ10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "PfVztXp-U7aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the application"
      ],
      "metadata": {
        "id": "UEQynddxdmu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose models\n",
        "\n",
        "base_model = 'decapoda-research/llama-7b-hf' #@param [\"decapoda-research/llama-7b-hf\", \"decapoda-research/llama-13b-hf\", \"decapoda-research/llama-30b-hf\"]\n",
        "finetuned_model = 'tloen/alpaca-lora-7b' #@param [\"tloen/alpaca-lora-7b\", \"chansung/alpaca-lora-13b\", \"chansung/koalpaca-lora-13b\", \"chansung/alpaca-lora-30b\"]"
      ],
      "metadata": {
        "id": "Wr5Iw5sLVnGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the application\n",
        "It will take some time since LLaMA weights are huge.\n",
        "\n",
        "Click the URL appeared in the Running on public URL: field from the log. That will bring you to a new browser tab which opens up the running application."
      ],
      "metadata": {
        "id": "rTzDmXmhdsaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py --base-url $base_model --ft-ckpt-url $finetuned_model --share"
      ],
      "metadata": {
        "id": "p0-TG3bcVrWH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}